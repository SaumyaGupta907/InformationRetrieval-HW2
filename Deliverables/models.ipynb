{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "731e49ee-141f-4376-880c-e782609d45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import requests\n",
    "import json\n",
    "import heapq\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e4b52b14-4f66-4f06-9c77-f0e74fda061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docTextMap = {}\n",
    "stemmer = PorterStemmer()\n",
    "applyStem = True\n",
    "folder = \"stemmed/\" if applyStem else \"unstemmed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e5add409-6ac8-4bd0-a9c3-b454938cfda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def requireStemming(token, applyStem):\n",
    "    if applyStem:\n",
    "        return stemmer.stem(token)\n",
    "    else:\n",
    "        return token\n",
    "def checkForStopWords(word):\n",
    "    return word.lower() in stopwords\n",
    "\n",
    "def is_number(word):\n",
    "    return word[0].isdigit() or word.isdigit() or is_float(word)\n",
    "\n",
    "def is_float(word):\n",
    "    try:\n",
    "        float_value = float(word)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "cd0bf334-facb-48e0-9e6d-dc23abab0cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stopwords: 531\n"
     ]
    }
   ],
   "source": [
    "stopWordPath = \"../Resources/IR_data/AP_DATA/stoplist.txt\"\n",
    "\n",
    "with open(stopWordPath) as file:\n",
    "    stopwords = file.readlines()\n",
    "    \n",
    "    for index, stopword in enumerate(stopwords):\n",
    "        stopwords[index] = stopword.split(\"\\n\")[0]\n",
    "        \n",
    "        \n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "extraPunc = [\"``\", \"'s'\", \"'\", \"''\"]\n",
    "[punctuations.append(el) for el in extraPunc]\n",
    "\n",
    "for p in punctuations:\n",
    "    stopwords.append(p)\n",
    "        \n",
    "print(f'Total number of stopwords: {len(stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4fdf6080-8134-484a-949c-af9c04f0c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for processing text\n",
    "def tokenize(text):\n",
    "    pattern = r\"\\w+(?:\\.?\\w)*\"\n",
    " \n",
    "    # Tokenize the text based on the pattern\n",
    "    words = re.findall(pattern, text.lower())\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e22a9274-4b39-456a-adc4-9653a996d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing, removing stop words and stemming the text\n",
    "def preprocess(text, applyStem):\n",
    "    tokens = tokenize(text)\n",
    "    words = []\n",
    "    for index, word in enumerate(tokens):\n",
    "        if not checkForStopWords(word) and not is_number(word):\n",
    "  #      if word.lower() not in checkForStopWords(word) and not is_number(word):\n",
    "            words.append(requireStemming(word, applyStem))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4f221e5f-713a-41be-942b-ca04268eb4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 ['alleg', 'corrupt', 'public', 'offici']\n",
      "59 ['weather', 'least', 'fatal', 'locat']\n",
      "56 ['prime', 'lend', 'rate', 'prime', 'rate']\n",
      "71 ['incurs', 'border', 'area', 'militari', 'forc', 'second', 'guerrilla', 'second']\n",
      "64 ['result', 'polit', 'hostag', 'take']\n",
      "62 ['militari', 'coup', 'd', 'etat', 'attempt']\n",
      "93 ['support', 'nation', 'rifl', 'associ', 'nra']\n",
      "99 ['develop', 'iran', 'contra', 'affair']\n",
      "58 ['rail', 'strike', 'ongo', 'rail', 'strike']\n",
      "77 ['poach', 'method', 'wildlif']\n",
      "54 ['contract', 'agreement', 'reserv', 'launch', 'commerci', 'satellit']\n",
      "87 ['current', 'offic', 'fail', 'institut']\n",
      "94 ['crime', 'aid', 'comput']\n",
      "100 ['non', 'communist', 'state', 'transfer', 'high', 'tech', 'good', 'technolog', 'nation']\n",
      "89 ['invest', 'opec', 'downstream']\n",
      "61 ['israel', 'iran', 'contra', 'affair']\n",
      "95 ['comput', 'applic', 'crime']\n",
      "68 ['concern', 'safeti', 'manufactur', 'worker', 'fine', 'diamet', 'fiber', 'product']\n",
      "57 ['mci', 'bell', 'system']\n",
      "97 ['fiber', 'optic', 'technolog']\n",
      "98 ['fiber', 'optic', 'equip']\n",
      "60 ['perform', 'salari', 'incent', 'pay', 'pay']\n",
      "80 ['presidenti', 'candid']\n",
      "63 ['machin', 'translat', 'system']\n",
      "91 ['weapon', 'system']\n"
     ]
    }
   ],
   "source": [
    "# Path of the query file\n",
    "queryFilePath = \"../Resources/IR_data/AP_DATA/query_desc.51-100.short.txt\"\n",
    "\n",
    "# Used to store the query ID with its respective Query\n",
    "queryIdMap = {}\n",
    "\n",
    "with open(queryFilePath) as file:\n",
    "    fileQuery = file.readlines()\n",
    "    for query in fileQuery:\n",
    "        query = query.split(\".\")\n",
    "        qid = query[0]\n",
    "        query = \"\".join(query[1:])   \n",
    "        queryIdMap[qid] = []\n",
    "        wList = preprocess(query, applyStem)\n",
    "        for w in wList:            \n",
    "            queryIdMap[qid].append(w)\n",
    "\n",
    "for qid, query in queryIdMap.items():\n",
    "    print(qid, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "25873f5f-60ce-47af-ba41-26c67e78d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to get the query details in the specified format.\n",
    "def formatLine(queryNo, doc_id, rank, score):\n",
    "    formatted_line = '{} Q0{}{} {} Exp\\n'.format(queryNo, doc_id, rank, score)\n",
    "    return formatted_line\n",
    "\n",
    "# USed to fetch the scores from the file.\n",
    "def getScores(output_file, queryNo, score_list):\n",
    "    with open(output_file, \"a\") as file:\n",
    "        for rank, (score, doc_id) in enumerate(score_list):\n",
    "            line = formatLine(queryNo, doc_id, rank, score)\n",
    "            file.write(line)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a7138af3-c0d3-4499-bb27-5dedf907a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgDocLen = 0\n",
    "doc_count = 0\n",
    "\n",
    "with open(f'../Resources/{folder}docTextLen.txt') as file:\n",
    "    for line in file:\n",
    "        docLen = json.loads(line)\n",
    "        for docId in docLen:\n",
    "            avgDocLen += docLen[docId]\n",
    "            doc_count += 1\n",
    "\n",
    "avgDocLen /= doc_count\n",
    "\n",
    "\n",
    "with open(f'../Resources/{folder}tokenIndexMap.txt') as file:\n",
    "    token_index_map = file.read()\n",
    "    \n",
    "token_index_map = json.loads(token_index_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5ea29cfd-464b-4f7a-a34b-3b595d4c15cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\R'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\R'\n",
      "C:\\Users\\sanje\\AppData\\Local\\Temp\\ipykernel_15356\\875913328.py:6: SyntaxWarning: invalid escape sequence '\\R'\n",
      "  with open('..\\Resources\\IR_data\\AP_DATA\\doclist.txt', 'r') as file:\n"
     ]
    }
   ],
   "source": [
    "# Initialize maps for token-index and docID-index\n",
    "docID_index_map = {}\n",
    "c = 0\n",
    "\n",
    "# Read docIDs from file and sort them\n",
    "with open('..\\Resources\\IR_data\\AP_DATA\\doclist.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        index, filename = line.split(maxsplit=1)\n",
    "        index = index.strip()  # Remove leading/trailing whitespace and newline character\n",
    "        filename = filename.strip()  # Remove leading/trailing whitespace and newline character\n",
    "        docID_index_map[' '+filename+' '] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "572463dc-7859-410c-8936-3a89455f6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read catalog file\n",
    "def openCatalog(path):\n",
    "    content = \"\"\n",
    "    \n",
    "    with open(f'../Resources/{folder}catalogs/{path}') as file:\n",
    "        content = file.readlines()\n",
    "        \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5dec07a2-56e6-4486-a25b-fc41ace5d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts catalog string to position list\n",
    "def catalogToPList(catalog):\n",
    "    positions = {}\n",
    "    for line in catalog:\n",
    "        parts = line.split()\n",
    "        if len(parts) > 1:\n",
    "            key = parts[0]\n",
    "            values = [int(parts[1]), int(parts[2])]\n",
    "            positions[key] = values\n",
    "\n",
    "    return positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a780519b-e7f2-4609-a7b5-d188556ddbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringToDictionary(str):\n",
    "    # Regular expression pattern to match docId and integers\n",
    "    p = r'(\\d+):(\\[\\d+(?:,\\d+)*\\])'\n",
    "\n",
    "    # Find all matches in the input string\n",
    "    match = re.finditer(p, str)\n",
    "\n",
    "    # Extract docId and integers from matches\n",
    "    extractedData = {}\n",
    "    for match in match:\n",
    "        doc_id = match.group(1)\n",
    "        nums = list(map(int, match.group(2)[1:-1].split(',')))  # Extract integers within []\n",
    "        extractedData[doc_id] = nums\n",
    "        \n",
    "    return extractedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "108a073e-ffe1-4377-982a-75c22ee0dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDict(key, position, path):\n",
    "    start, length = position\n",
    "    \n",
    "    # Read content from file with specific start and length\n",
    "    with open(f'../Resources/{folder}/invertedIndex/{path}') as file:\n",
    "        file.seek(start)\n",
    "        line = file.read(length + 1)\n",
    "        \n",
    "    token = line.split(\":\")[0]\n",
    "\n",
    "    # Extract the catalog string\n",
    "    catalog_string = line[len(token)+2:-1]\n",
    "\n",
    "    # Convert catalog string to dictionary\n",
    "    dd = stringToDictionary(catalog_string)\n",
    "    \n",
    "    # Create a dictionary with the token as key and the converted catalog as value\n",
    "    updatedDD = {token: dd}\n",
    "    \n",
    "    return updatedDD\n",
    "\n",
    "'''# Pre-computes query catalog\n",
    "catalog = openCatalog(\"0.txt\")\n",
    "catalogPos = catalogToPList(catalog)\n",
    "# vocabulary size: unique terms in the collection\n",
    "V = len(catalogPos.keys())\n",
    "queryCatalogDict = {}\n",
    "for qid, query in queryIdMap.items():\n",
    "    for queryWord in query:\n",
    "        if queryWord in queryCatalogDict:\n",
    "            continue        \n",
    "        token = str(token_index_map[queryWord])\n",
    "        queryWPos = catalogPos[token]\n",
    "        catalogDict = extractDict(token, queryWPos, \"0.txt\")\n",
    "        queryCatalogDict[token] = catalogDict[token] '''\n",
    "\n",
    "# Pre-computes query catalog\n",
    "catalog = openCatalog(\"0.txt\")\n",
    "catalogPos = catalogToPList(catalog)\n",
    "# vocabulary size: unique terms in the collection\n",
    "Vocab = len(catalogPos.keys())\n",
    "queryCatalogDict = {}\n",
    "\n",
    "# Using setdefault to avoid unnecessary if conditions\n",
    "for qid, query in queryIdMap.items():\n",
    "    for queryWord in query:\n",
    "        token = str(token_index_map[queryWord])\n",
    "        queryWPos = catalogPos.get(token, [])\n",
    "        catalogDict = extractDict(token, queryWPos, \"0.txt\")\n",
    "        queryCatalogDict.setdefault(token, catalogDict.get(token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaac47e-a582-4219-b714-01a728886173",
   "metadata": {},
   "source": [
    "### OkapiTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ba913301-a531-4c42-ad9b-060a448e2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapiTF(path, queryIdMap, queryCatalogDict):\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # Initialize the OkapiTF Dictionary\n",
    "    okapiTf = {qid: [] for qid in queryIdMap.keys()}\n",
    "\n",
    "    for index, docId in enumerate(docLen.keys()):\n",
    "        if index % 10000 == 0:\n",
    "            print(f'On {index}th doc')\n",
    "\n",
    "        for qid, query in queryIdMap.items():\n",
    "            doc_Okapi = 0\n",
    "\n",
    "            for queryWord in query:\n",
    "                tf = 0\n",
    "\n",
    "                token = str(token_index_map[queryWord])\n",
    "\n",
    "                if docID_index_map[docId] in queryCatalogDict[token]:\n",
    "                    tf = len(queryCatalogDict[token][docID_index_map[docId]])\n",
    "\n",
    "                doc_Okapi += tf / (tf + 0.5 + 1.5 * (docLen[docId] / avgDocLen))\n",
    "\n",
    "            heapq.heappush(okapiTf[qid], (doc_Okapi, docId))\n",
    "\n",
    "\n",
    "    for index, e in okapiTf.items():\n",
    "        e = sorted(e, reverse=True)[:1000]\n",
    "        getScores(path, index, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ff183186-58e4-45c3-b2e0-f8c441142da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 0th doc\n",
      "On 10000th doc\n",
      "On 20000th doc\n",
      "On 30000th doc\n",
      "On 40000th doc\n",
      "On 50000th doc\n",
      "On 60000th doc\n",
      "On 70000th doc\n",
      "On 80000th doc\n"
     ]
    }
   ],
   "source": [
    "okapiTF(f'./output/{folder}okapiTf.txt', queryIdMap, queryCatalogDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a2415-0e15-4958-9007-911473bc816e",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5b51ff89-18ad-42e7-b06b-6defb3971432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfIdf(path, queryIdMap, queryCatalogDict):\n",
    "\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        \n",
    "    # total number of documents in the corpus\n",
    "    D = len(docLen)\n",
    "\n",
    "    # map of term frequencies for a word and all documents\n",
    "    tfIdf = {}\n",
    "\n",
    "    # Initializes the tfIdf map\n",
    "    for qid in queryIdMap.keys():\n",
    "        tfIdf[qid] = []\n",
    "\n",
    "    for index, docId in enumerate(docLen.keys()):\n",
    "        if index % 10000 == 0:\n",
    "            print(f'On {index}th doc')\n",
    "\n",
    "        for qid, query in queryIdMap.items():\n",
    "            docTfIdf = 0\n",
    "            \n",
    "            currentPositionLists = []\n",
    "            \n",
    "            for queryWord in query:\n",
    "                token = str(token_index_map[queryWord])\n",
    "                \n",
    "                if docID_index_map[docId] in queryCatalogDict[token]:\n",
    "                    currentPositionLists.append(queryCatalogDict[token][docID_index_map[docId]])\n",
    "                else:\n",
    "                    currentPositionLists.append([])\n",
    "            \n",
    "            for queryWord in query:\n",
    "                tf = 0\n",
    "                \n",
    "                token = str(token_index_map[queryWord])\n",
    "                \n",
    "                dfw = len(queryCatalogDict[token].keys())\n",
    "                \n",
    "                if docID_index_map[docId] in queryCatalogDict[token]:\n",
    "                    tf = len(queryCatalogDict[token][docID_index_map[docId]])\n",
    "                \n",
    "                docTfIdf += tf / (tf + 0.5 + 1.5*(docLen[docId] / avgDocLen)) * math.log(D / dfw)\n",
    "            \n",
    "            heapq.heappush(tfIdf[qid], (docTfIdf, docId))\n",
    "\n",
    "\n",
    "    for index, e in tfIdf.items():\n",
    "        e = sorted(e, reverse=True)[:1000]\n",
    "        getScores(path, index, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e4a19f4c-d13e-4cc9-95d7-efdeb9a3481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 0th doc\n",
      "On 10000th doc\n",
      "On 20000th doc\n",
      "On 30000th doc\n",
      "On 40000th doc\n",
      "On 50000th doc\n",
      "On 60000th doc\n",
      "On 70000th doc\n",
      "On 80000th doc\n"
     ]
    }
   ],
   "source": [
    "tfIdf(f'./output/{folder}tfIdf.txt', queryIdMap, queryCatalogDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11854c8-461f-4ebe-b6d7-2c6133ac7511",
   "metadata": {},
   "source": [
    "## Okapi BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "50dca121-3ec2-4b25-b9f1-afbf9d2867df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get1stTerm(D, df_w):\n",
    "    return math.log((D + 0.5) / (df_w + 0.5))\n",
    "\n",
    "def get2ndTerm(tf_wd, k1, b, lenD, avgLenD):\n",
    "    return (tf_wd + k1 * tf_wd) / (tf_wd + k1 * ((1 - b) + b * (lenD / avgLenD)))\n",
    "\n",
    "def get3rdTerm(tf_wq, k2):\n",
    "    return (tf_wq + k2 * tf_wq) / (tf_wq + k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "035f25c4-181d-441f-ade1-77ac553e8b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(path, queryIdMap, queryCatalogDict):\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    k1 = 1.2\n",
    "    k2 = 100\n",
    "    b = 0.75\n",
    "    \n",
    "    # Count of total docs overall\n",
    "    D = len(docLen)\n",
    "    \n",
    "    # Initialize the dictionary to store Okapi BM25 scores for each query\n",
    "    okapi_bm25 = {qid: [] for qid in queryIdMap.keys()}\n",
    "    \n",
    "    for index, docId in enumerate(docLen.keys()):\n",
    "        if index % 10000 == 0:\n",
    "            print(f'On {index}th doc')\n",
    "\n",
    "        for qid, query in queryIdMap.items():\n",
    "            # Map for containing frequencies of words in a given query\n",
    "            tf_wqs = {}\n",
    "\n",
    "            for word in query:\n",
    "                if word not in tf_wqs:\n",
    "                    tf_wqs[word] = 0\n",
    "                tf_wqs[word] += 1\n",
    "\n",
    "            doc_okapi_bm25 = 0\n",
    "            \n",
    "            for word, tf_wq in tf_wqs.items():\n",
    "                tf_wd = 0\n",
    "                \n",
    "                token = str(token_index_map.get(word, None))\n",
    "                \n",
    "                if token is not None:\n",
    "                    df_w = len(queryCatalogDict.get(token, {}).keys())\n",
    "                    \n",
    "                    if docID_index_map.get(docId, None) in queryCatalogDict.get(token, {}):\n",
    "                        tf_wd = len(queryCatalogDict[token].get(docID_index_map[docId], []))\n",
    "                    \n",
    "                    doc_okapi_bm25 += get1stTerm(D, df_w) * get2ndTerm(tf_wd, k1, b, docLen.get(docId, 0), avgDocLen) * get3rdTerm(tf_wq, k2)\n",
    "                \n",
    "            heapq.heappush(okapi_bm25[qid], (doc_okapi_bm25, docId))\n",
    "    for index, e in okapi_bm25.items():\n",
    "        e = sorted(e, reverse=True)[:1000]\n",
    "        getScores(path, index, e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2b6bcd86-c381-4e89-bbeb-bca1eaf25dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 0th doc\n",
      "On 10000th doc\n",
      "On 20000th doc\n",
      "On 30000th doc\n",
      "On 40000th doc\n",
      "On 50000th doc\n",
      "On 60000th doc\n",
      "On 70000th doc\n",
      "On 80000th doc\n"
     ]
    }
   ],
   "source": [
    "bm25(f'./output/{folder}bm25-decompressed.txt', queryIdMap, queryCatalogDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad854f65-bffe-4da6-a8d6-33160acb6496",
   "metadata": {},
   "source": [
    "## Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "24784ff1-a468-40ab-81f5-31a05ed04fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplaceSmoothing(path, queryIdMap):\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    # Initialise laplace dictionary    \n",
    "    laplace = {qid: [] for qid in queryIdMap.keys()}\n",
    "\n",
    "    for docNo in docLen.keys():\n",
    "\n",
    "        for qid, query in queryIdMap.items():\n",
    "            calulate = 0\n",
    "\n",
    "            for queryWord in query:\n",
    "                tf = 0\n",
    "\n",
    "                token = str(token_index_map.get(queryWord, None))\n",
    "\n",
    "                if token is not None and docID_index_map.get(docNo, None) in queryCatalogDict.get(token, {}):\n",
    "                    tf = len(queryCatalogDict[token][docID_index_map[docNo]])\n",
    "\n",
    "                calulate += -1000 if tf == 0 else math.log((tf + 1) / (docLen.get(docNo, 0) + Vocab))\n",
    "\n",
    "            heapq.heappush(laplace[qid], (calulate, docNo))\n",
    "\n",
    "    for index, e in laplace.items():\n",
    "        e = sorted(e, reverse=True)[:1000]\n",
    "        getScores(path, index, e)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "623e6b13-a5a4-4ad2-b103-e35a02bd6b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 0th doc\n",
      "On 10000th doc\n",
      "On 20000th doc\n",
      "On 30000th doc\n",
      "On 40000th doc\n",
      "On 50000th doc\n",
      "On 60000th doc\n",
      "On 70000th doc\n",
      "On 80000th doc\n"
     ]
    }
   ],
   "source": [
    "laplaceSmoothing(f'./output/{folder}laplace3.txt', queryIdMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3d219cc8-d1e1-4726-866c-c3e0446791f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_indices_at_end(position_lists, indices):\n",
    "    for i in range(len(indices)):\n",
    "        if indices[i] < len(position_lists[i]) - 1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_min_span(position_lists):\n",
    "    for pos_list in position_lists:\n",
    "        if len(pos_list) == 0:\n",
    "            return 1e5\n",
    "    \n",
    "    min_span = 1e5\n",
    "    indices = [0] * len(position_lists)\n",
    "\n",
    "    while not all_indices_at_end(position_lists, indices):\n",
    "        min_val = 1e9\n",
    "        next_index = find_next_index(position_lists, indices)\n",
    "\n",
    "        min_span = update_min_span(position_lists, indices, min_span)\n",
    "\n",
    "        indices[next_index] += 1\n",
    "\n",
    "    return min_span\n",
    "\n",
    "def find_next_index(position_lists, indices):\n",
    "    min_val = 1e9\n",
    "    next_index = -1\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        index = indices[i]\n",
    "\n",
    "        if min_val > position_lists[i][index] and index + 1 != len(position_lists[i]):\n",
    "            min_val = position_lists[i][index]\n",
    "            next_index = i\n",
    "\n",
    "    return next_index\n",
    "\n",
    "def update_min_span(position_lists, indices, current_min_span):\n",
    "    min_element, max_element = 1e9, -1\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        min_element = min(min_element, position_lists[i][indices[i]])\n",
    "        max_element = max(max_element, position_lists[i][indices[i]])\n",
    "\n",
    "    return min(current_min_span, abs(max_element - min_element))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "aec3af31-6c22-4166-8e1a-df7696cc17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfIdfProximitySearch(path, queryIdMap, queryCatalogDict):\n",
    "\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        \n",
    "    # total number of documents in the corpus\n",
    "    D = len(docLen)\n",
    "\n",
    "    # map of term frequencies for a word and all documents\n",
    "    tfIdf = {}\n",
    "    \n",
    "    _lambda = 0.8\n",
    "\n",
    "    # Initializes the tfIdf map\n",
    "    for qid in queryIdMap.keys():\n",
    "        tfIdf[qid] = []\n",
    "\n",
    "    for index, docId in enumerate(docLen.keys()):\n",
    "        if index % 10000 == 0:\n",
    "            print(f'On {index}th doc')\n",
    "\n",
    "        for qid, query in queryIdMap.items():\n",
    "            docTfIdf = 0\n",
    "            \n",
    "            currentPositionLists = []\n",
    "            \n",
    "            for queryWord in query:\n",
    "                token = str(token_index_map[queryWord])\n",
    "                \n",
    "                if docID_index_map[docId] in queryCatalogDict[token]:\n",
    "                    currentPositionLists.append(queryCatalogDict[token][docID_index_map[docId]])\n",
    "                else:\n",
    "                    currentPositionLists.append([])\n",
    "                                           \n",
    "            minSpan = find_min_span(currentPositionLists)    \n",
    "            \n",
    "            proximityScore = math.log(0.01 + math.exp(-minSpan))\n",
    "            \n",
    "            for queryWord in query:\n",
    "                tf = 0\n",
    "                \n",
    "                token = str(token_index_map[queryWord])\n",
    "                \n",
    "                dfw = len(queryCatalogDict[token].keys())\n",
    "                \n",
    "                if docID_index_map[docId] in queryCatalogDict[token]:\n",
    "                    tf = len(queryCatalogDict[token][docID_index_map[docId]])\n",
    "                \n",
    "                docTfIdf += tf / (tf + 0.5 + 1.5*(docLen[docId] / avgDocLen)) * math.log(D / dfw)\n",
    "            \n",
    "            heapq.heappush(tfIdf[qid], (docTfIdf + proximityScore, docId))\n",
    "\n",
    "\n",
    "    for index, e in tfIdf.items():\n",
    "        e = sorted(e, reverse=True)[:1000]\n",
    "        getScores(path, index, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8a69a32b-1a89-433f-94cc-530708abf320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 0th doc\n",
      "On 10000th doc\n",
      "On 20000th doc\n",
      "On 30000th doc\n",
      "On 40000th doc\n",
      "On 50000th doc\n",
      "On 60000th doc\n",
      "On 70000th doc\n",
      "On 80000th doc\n"
     ]
    }
   ],
   "source": [
    "tfIdfProximitySearch(f'./output/{folder}tfIdfProximitySearch.txt', queryIdMap, queryCatalogDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690c3c2-4875-4936-b472-677ef4295f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
