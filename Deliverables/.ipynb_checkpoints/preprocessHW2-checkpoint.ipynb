{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c525d4b-165e-4290-98f8-91a9f41e4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import chardet\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df7908-2bdc-416f-8eb8-9e500e7e7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "docTextMap = {}\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e2383-8667-4d9e-afe7-fd82b4a14511",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46ad9b-b127-40b5-8409-7c72cfbd6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to remove numbers during pre-processing\n",
    "\n",
    "def is_number(word):\n",
    "    return word[0].isdigit() or word.isdigit() or is_float(word)\n",
    "\n",
    "def is_float(word):\n",
    "    try:\n",
    "        float_value = float(word)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "def checkForStopWords(word):\n",
    "    return word.lower() in stopwords\n",
    "def requireStemming(token, applyStem):\n",
    "    if applyStem:\n",
    "        return ps.stem(token)\n",
    "    else:\n",
    "        return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c413f1b5-ddfd-44f3-8124-9950ee5724ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stopwords: 454\n"
     ]
    }
   ],
   "source": [
    "stopWordPath = \"../Resources/stoplist.txt\"\n",
    "\n",
    "with open(stopWordPath) as file:\n",
    "    stopwords = file.readlines()\n",
    "    \n",
    "    for index, stopword in enumerate(stopwords):\n",
    "        stopwords[index] = stopword.split(\"\\n\")[0]\n",
    "        \n",
    "        \n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "extraPunc = [\"``\", \"'s'\", \"'\", \"''\"]\n",
    "[punctuations.append(el) for el in extraPunc]\n",
    "\n",
    "for p in punctuations:\n",
    "    stopwords.append(p)\n",
    "        \n",
    "print(f'Total number of stopwords: {len(stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b11c6066-b3aa-4afe-8395-061408a21fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for processing text\n",
    "def tokenize(text):\n",
    "    pattern = r\"\\w+(?:\\.?\\w)*\"\n",
    " \n",
    "    # Tokenize the text based on the pattern\n",
    "    words = re.findall(pattern, text.lower())\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf8f6c74-a745-4067-9b47-8fb2af44fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing, removing stop words and stemming the text\n",
    "def preprocess(text, applyStem):\n",
    "    tokens = tokenize(text)\n",
    "    words = []\n",
    "\n",
    "    for word in tokens:\n",
    "        if word not in checkForStopWords(word) and not is_number(word):\n",
    "            words.append(requireStemming(word,applyStem))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc5053b2-5592-445a-b313-1def3dfe2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map to store docNo and its corresponding text\n",
    "docTextMap = {}\n",
    "# Map for storing docId and length of its text\n",
    "docTextLen = {}\n",
    "\n",
    "def extractData(path, applyStem):\n",
    "    doc = re.compile(r'<DOC>(.*?)</DOC>', re.DOTALL)\n",
    "\n",
    "    with open(path, 'rb') as rawfile:\n",
    "        result = chardet.detect(rawfile.read())\n",
    "        encoding = result['encoding']\n",
    "\n",
    "    with open(path, 'r', encoding=encoding) as file:\n",
    "        file_content = file.read()\n",
    "    \n",
    "    # Read the content of the file\n",
    "    with open(path, 'r') as file:\n",
    "        file_content = file.read()\n",
    "\n",
    "    docs = re.findall(doc, file_content)\n",
    "    \n",
    "    for match in docs:\n",
    "        docNo = re.findall(r'<DOCNO>(.*?)</DOCNO>', match, re.DOTALL)[0]\n",
    "        text = re.findall(r'<TEXT>(.*?)</TEXT>', match, re.DOTALL)\n",
    "               \n",
    "        text = \"\".join(text).lower()\n",
    "        \n",
    "        docTextMap[docNo] = preprocess(text, applyStem)\n",
    "\n",
    "        docTextLen[docNo] = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dc2b305-23c6-4ecc-b0be-273b53af5459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap890101\n",
      "ap890102\n",
      "ap890103\n",
      "ap890104\n",
      "ap890105\n",
      "ap890106\n",
      "ap890107\n",
      "ap890108\n",
      "ap890109\n",
      "ap890110\n",
      "ap890111\n",
      "ap890112\n",
      "ap890113\n",
      "ap890114\n",
      "ap890115\n",
      "ap890116\n",
      "ap890117\n",
      "ap890118\n",
      "ap890119\n",
      "ap890120\n",
      "ap890121\n",
      "ap890122\n",
      "ap890123\n",
      "ap890124\n",
      "ap890125\n",
      "ap890126\n",
      "ap890127\n",
      "ap890128\n",
      "ap890129\n",
      "ap890130\n",
      "ap890131\n",
      "ap890201\n",
      "ap890202\n",
      "ap890203\n",
      "ap890204\n",
      "ap890205\n",
      "ap890206\n",
      "ap890207\n",
      "ap890208\n",
      "ap890209\n",
      "ap890210\n",
      "ap890211\n",
      "ap890212\n",
      "ap890213\n",
      "ap890214\n",
      "ap890215\n",
      "ap890216\n",
      "ap890217\n",
      "ap890218\n",
      "ap890219\n",
      "ap890220\n",
      "ap890221\n",
      "ap890222\n",
      "ap890223\n",
      "ap890224\n",
      "ap890225\n",
      "ap890226\n",
      "ap890227\n",
      "ap890228\n",
      "ap890301\n",
      "ap890302\n",
      "ap890303\n",
      "ap890304\n",
      "ap890305\n",
      "ap890306\n",
      "ap890307\n",
      "ap890308\n",
      "ap890309\n",
      "ap890310\n",
      "ap890311\n",
      "ap890312\n",
      "ap890313\n",
      "ap890314\n",
      "ap890315\n",
      "ap890316\n",
      "ap890317\n",
      "ap890318\n",
      "ap890319\n",
      "ap890320\n",
      "ap890321\n",
      "ap890322\n",
      "ap890323\n",
      "ap890324\n",
      "ap890325\n",
      "ap890326\n",
      "ap890327\n",
      "ap890328\n",
      "ap890329\n",
      "ap890330\n",
      "ap890401\n",
      "ap890402\n",
      "ap890403\n",
      "ap890404\n",
      "ap890405\n",
      "ap890406\n",
      "ap890407\n",
      "ap890408\n",
      "ap890409\n",
      "ap890410\n",
      "ap890411\n",
      "ap890412\n",
      "ap890413\n",
      "ap890414\n",
      "ap890415\n",
      "ap890416\n",
      "ap890417\n",
      "ap890418\n",
      "ap890419\n",
      "ap890420\n",
      "ap890421\n",
      "ap890422\n",
      "ap890423\n",
      "ap890424\n",
      "ap890425\n",
      "ap890426\n",
      "ap890427\n",
      "ap890428\n",
      "ap890429\n",
      "ap890430\n",
      "ap890501\n",
      "ap890502\n",
      "ap890503\n",
      "ap890504\n",
      "ap890505\n",
      "ap890506\n",
      "ap890507\n",
      "ap890508\n",
      "ap890509\n",
      "ap890510\n",
      "ap890511\n",
      "ap890512\n",
      "ap890513\n",
      "ap890514\n",
      "ap890515\n",
      "ap890516\n",
      "ap890517\n",
      "ap890518\n",
      "ap890519\n",
      "ap890520\n",
      "ap890521\n",
      "ap890522\n",
      "ap890523\n",
      "ap890524\n",
      "ap890525\n",
      "ap890526\n",
      "ap890527\n",
      "ap890528\n",
      "ap890529\n",
      "ap890530\n",
      "ap890531\n",
      "ap890601\n",
      "ap890602\n",
      "ap890603\n",
      "ap890604\n",
      "ap890605\n",
      "ap890606\n",
      "ap890607\n",
      "ap890608\n",
      "ap890609\n",
      "ap890610\n",
      "ap890611\n",
      "ap890612\n",
      "ap890613\n",
      "ap890614\n",
      "ap890615\n",
      "ap890616\n",
      "ap890617\n",
      "ap890618\n",
      "ap890619\n",
      "ap890620\n",
      "ap890621\n",
      "ap890622\n",
      "ap890623\n",
      "ap890624\n",
      "ap890625\n",
      "ap890626\n",
      "ap890627\n",
      "ap890628\n",
      "ap890629\n",
      "ap890630\n",
      "ap890701\n",
      "ap890702\n",
      "ap890703\n",
      "ap890704\n",
      "ap890705\n",
      "ap890706\n",
      "ap890707\n",
      "ap890708\n",
      "ap890709\n",
      "ap890710\n",
      "ap890711\n",
      "ap890712\n",
      "ap890713\n",
      "ap890714\n",
      "ap890715\n",
      "ap890716\n",
      "ap890717\n",
      "ap890718\n",
      "ap890719\n",
      "ap890720\n",
      "ap890721\n",
      "ap890722\n",
      "ap890723\n",
      "ap890724\n",
      "ap890725\n",
      "ap890726\n",
      "ap890727\n",
      "ap890728\n",
      "ap890729\n",
      "ap890730\n",
      "ap890731\n",
      "ap890801\n",
      "ap890802\n",
      "ap890803\n",
      "ap890804\n",
      "ap890805\n",
      "ap890806\n",
      "ap890807\n",
      "ap890808\n",
      "ap890809\n",
      "ap890810\n",
      "ap890811\n",
      "ap890812\n",
      "ap890813\n",
      "ap890814\n",
      "ap890815\n",
      "ap890816\n",
      "ap890817\n",
      "ap890818\n",
      "ap890819\n",
      "ap890820\n",
      "ap890821\n",
      "ap890822\n",
      "ap890823\n",
      "ap890824\n",
      "ap890825\n",
      "ap890826\n",
      "ap890827\n",
      "ap890828\n",
      "ap890829\n",
      "ap890830\n",
      "ap890831\n",
      "ap890901\n",
      "ap890902\n",
      "ap890903\n",
      "ap890904\n",
      "ap890905\n",
      "ap890906\n",
      "ap890907\n",
      "ap890908\n",
      "ap890909\n",
      "ap890910\n",
      "ap890911\n",
      "ap890912\n",
      "ap890913\n",
      "ap890914\n",
      "ap890915\n",
      "ap890916\n",
      "ap890917\n",
      "ap890918\n",
      "ap890919\n",
      "ap890920\n",
      "ap890921\n",
      "ap890922\n",
      "ap890923\n",
      "ap890924\n",
      "ap890925\n",
      "ap890926\n",
      "ap890927\n",
      "ap890928\n",
      "ap890929\n",
      "ap890930\n",
      "ap891001\n",
      "ap891002\n",
      "ap891003\n",
      "ap891004\n",
      "ap891005\n",
      "ap891006\n",
      "ap891007\n",
      "ap891008\n",
      "ap891009\n",
      "ap891010\n",
      "ap891011\n",
      "ap891012\n",
      "ap891013\n",
      "ap891014\n",
      "ap891015\n",
      "ap891016\n",
      "ap891017\n",
      "ap891018\n",
      "ap891019\n",
      "ap891020\n",
      "ap891021\n",
      "ap891022\n",
      "ap891023\n",
      "ap891024\n",
      "ap891025\n",
      "ap891026\n",
      "ap891027\n",
      "ap891028\n",
      "ap891029\n",
      "ap891030\n",
      "ap891031\n",
      "ap891101\n",
      "ap891102\n",
      "ap891103\n",
      "ap891104\n",
      "ap891105\n",
      "ap891106\n",
      "ap891107\n",
      "ap891108\n",
      "ap891109\n",
      "ap891110\n",
      "ap891111\n",
      "ap891112\n",
      "ap891113\n",
      "ap891114\n",
      "ap891115\n",
      "ap891116\n",
      "ap891117\n",
      "ap891118\n",
      "ap891119\n",
      "ap891120\n",
      "ap891121\n",
      "ap891122\n",
      "ap891123\n",
      "ap891124\n",
      "ap891125\n",
      "ap891126\n",
      "ap891127\n",
      "ap891128\n",
      "ap891129\n",
      "ap891130\n",
      "ap891201\n",
      "ap891202\n",
      "ap891203\n",
      "ap891204\n",
      "ap891205\n",
      "ap891206\n",
      "ap891207\n",
      "ap891208\n",
      "ap891209\n",
      "ap891210\n",
      "ap891211\n",
      "ap891212\n",
      "ap891213\n",
      "ap891214\n",
      "ap891215\n",
      "ap891216\n",
      "ap891217\n",
      "ap891218\n",
      "ap891219\n",
      "ap891220\n",
      "ap891221\n",
      "ap891222\n",
      "ap891223\n",
      "ap891224\n",
      "ap891225\n",
      "ap891226\n",
      "ap891227\n",
      "ap891228\n",
      "ap891229\n",
      "ap891230\n",
      "ap891231\n",
      "readme\n"
     ]
    }
   ],
   "source": [
    "#Retrieving the files from ap98_collection\n",
    "folder = \"../Resources/IR_data/AP_DATA/ap89_collection\"\n",
    "\n",
    "for i, file in sorted(os.listdir(folder)):\n",
    "    print(file)\n",
    "    if i % 20 == 0:\n",
    "        print(f'Index:{i}')\n",
    "        \n",
    "    if file != \"readme\":\n",
    "        file_path = os.path.join(folder, file)\n",
    "        extractData(file_path, applyStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0f89706-ffaf-4a36-b321-ba6053d22779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get token and term position map\n",
    "token_index_map = {}\n",
    "c = 0\n",
    "token_position_dict = {}\n",
    "\n",
    "for docNo in docTextMap.keys():\n",
    "    currentDict = {}\n",
    "    for index, token in enumerate(docTextMap[docNo]):      \n",
    "        if token not in currentDict:\n",
    "            currentDict[token] = []\n",
    "            \n",
    "        currentDict[token].append(index + 1)\n",
    "\n",
    "        if token not in token_index_map:\n",
    "            c = c + 1\n",
    "            token_index_map[token] = c\n",
    "            \n",
    "\n",
    "    token_position_dict[docNo] = currentDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a13e8522-7000-4a73-afad-370f3693aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFile(data, path):\n",
    "    if doStem:\n",
    "        folderName = \"stemmed/\"\n",
    "    else:\n",
    "        folderName = \"unstemmed/\"\n",
    "        \n",
    "    with open(os.path.join(f'../Resources/{folderName}', path), \"w\") as file:\n",
    "        file.write(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5284430e-ef2f-419e-b65b-e0e18e6996a0",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8c31008-d2c6-40a7-a73e-29fa08ab8da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\R'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\R'\n",
      "C:\\Users\\sanje\\AppData\\Local\\Temp\\ipykernel_5648\\3031223229.py:6: SyntaxWarning: invalid escape sequence '\\R'\n",
      "  with open('..\\Resources\\IR_data\\AP_DATA\\doclist.txt', 'r') as file:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize maps for token-index and docID-index\n",
    "docID_index_map = {}\n",
    "c = 0\n",
    "\n",
    "# Read docIDs from file and sort them\n",
    "with open('..\\Resources\\IR_data\\AP_DATA\\doclist.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        index, filename = line.split(maxsplit=1)\n",
    "        index = index.strip()  # Remove leading/trailing whitespace and newline character\n",
    "        filename = filename.strip()  # Remove leading/trailing whitespace and newline character\n",
    "        docID_index_map[' '+filename+' '] = index\n",
    "\n",
    " '''   with open('..\\Resources\\IR_data\\AP_DATA\\doclist.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip().split(' ')\n",
    "        try:\n",
    "            if len(line) >= 2:\n",
    "                docId = ' ' + line[1] + ' '\n",
    "                docIdCounterMap[docId] = line[0]\n",
    "        except IndexError:\n",
    "            pass        '''\n",
    "\n",
    "\n",
    "writeFile(json.dumps(docTextLen), \"docTextLen.txt\");\n",
    "writeFile(json.dumps(token_index_map), \"tokenIndexMap.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2aff2e-0230-4a8c-a8ad-e62608c9665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverted Index to string, inorder to store it in a text file.\n",
    "def toString(invertedIndex):\n",
    "    indexValue = \"\"\n",
    "\n",
    "    for key1 in invertedIndex.keys():\n",
    "        content = f\"{key1}:{{\"\n",
    "        inner_content = [\n",
    "            f\"{key2}:[{','.join(map(str, invertedIndex[key1][key2]))}]\"\n",
    "            for key2 in invertedIndex[key1].keys()\n",
    "        ]\n",
    "        content += ','.join(inner_content)\n",
    "        content += \"}\"\n",
    "        indexValue += content\n",
    "\n",
    "    return indexValue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6c669-920b-4995-a918-f88373e40a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCatalogString(str):\n",
    "    catalog = \"\"    \n",
    "    startIndex = 0\n",
    "    i = 0\n",
    "    while i < len(str):\n",
    "        token = \"\"\n",
    "        length = 0\n",
    "        while i < len(str) and str[i] != ':':\n",
    "            length += 1\n",
    "            token += str[i]\n",
    "            i += 1\n",
    "        while i < len(str) and str[i] != '}':\n",
    "            length += 1\n",
    "            i += 1\n",
    "        i += 1\n",
    "        catalog += f'{token} {startIndex} {length}\\n'\n",
    "        startIndex += length + 1        \n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98d1182a-4771-4309-98c6-e4d6b5d08968",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def getCatalog(inverted_index):\n",
    "    # Convert the inverted index map into String\n",
    "    convertToString = json.dumps(inverted_index)\n",
    " \n",
    "    #Initialise tokens\n",
    "    tokens = inverted_index.keys()\n",
    "    catalog = \"\"\n",
    " \n",
    "    # Looping through the tokens\n",
    "    for token in tokens:\n",
    "        #Find the required token\n",
    "        start = convertToString.find(token) - 1\n",
    "        #Get the ending of the token\n",
    "        #If it is the end of the String i.e. -1, then return (length - 1)\n",
    "        # Else return till \"}\" is found\n",
    "        end = len(convertToString) - 1 if convertToString.find(\"},\", start) == -1 else convertToString.find(\"},\", start) + 1\n",
    "        catalog += f'{token} {start} {end-start+1}\\n'\n",
    "    return catalog '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd48dc-1666-44dd-8108-ad72f3d56d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# Get inverted index\n",
    "docNos = sorted(list(docID_index_map.keys()))\n",
    "\n",
    "for startIndex in range(0, len(docNos), 1000):\n",
    "    convert = {}\n",
    "\n",
    "    endIndex = min(len(docNos) - 1, startIndex + 999)\n",
    "\n",
    "    for index in range(startIndex, endIndex + 1):\n",
    "        docNo = docNos[index]\n",
    "\n",
    "        for token in token_position_dict[docId]:\n",
    "            positionList = token_position_dict[docId][token]\n",
    "\n",
    "            c = token_index_map[token]\n",
    "            \n",
    "            if c not in invertedIndex:\n",
    "                convert[c] = {}\n",
    "                \n",
    "            if c not in inverted_index:\n",
    "                inverted_index[c] = {}\n",
    "                \n",
    "            convert[c][docID_index_map[docId]] = positionList\n",
    "            inverted_index[c][docID_index_map[docId]] = positionList\n",
    "\n",
    "    fileNo = int(start / 1000)\n",
    "    \n",
    "    if int(start / 1000) % 20 == 0:\n",
    "        print(f'Writing into {fileNo}.txt')\n",
    "        \n",
    "    # Write inverted index\n",
    "    s = toString(convert)\n",
    "    writeFile(s, f'invertedIndex/{fileNo}.txt')\n",
    "    \n",
    "    catalogString = getCatalogString(s)\n",
    "    \n",
    "    writeFile(catalogString, f'catalogs/{fileNo}.txt')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701e7a3-58b4-48f0-a49e-837879770c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging\n",
    "\n",
    "'''def getCData(path):\n",
    "    data = {}\n",
    "    \n",
    "    with open(path) as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            key = parts[0]\n",
    "            values = [int(parts[1]), int(parts[2])]\n",
    "            data[key] = values\n",
    "        \n",
    "    return data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302540d-6c83-480c-a777-431681167839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts catalog string to position list\n",
    "def catalogToPList(catalog):\n",
    "    positions = {}\n",
    "    for line in catalog:\n",
    "        parts = line.split()\n",
    "        if len(parts) > 1:\n",
    "            key = parts[0]\n",
    "            values = [int(parts[1]), int(parts[2])]\n",
    "            positions[key] = values\n",
    "\n",
    "    return positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6725a3d-5617-4ff6-8c23-15a7f3410b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"stemmed/\" if doStem else \"unstemmed/\"\n",
    "# Read catalog file\n",
    "def openCatalog(path):\n",
    "    content = \"\"\n",
    "    \n",
    "    with open(f'../Resources/{folder}catalogs/{path}') as file:\n",
    "        content = file.readlines()\n",
    "        \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678f240-dd4b-4fad-bba1-854d1f401a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def stringToDictionary(inp):\n",
    "    # Regular expression pattern to match docId and integers\n",
    "    pattern = r'(\\d+):(\\[\\d+(?:,\\d+)*\\])'\n",
    "\n",
    "    # Find all matches in the input string\n",
    "    matches = re.findall(pattern, inp)\n",
    "\n",
    "    # Extract docId and integers from matches\n",
    "    dd = {}\n",
    "    \n",
    "    for match in matches:\n",
    "        doc_id = match[0]\n",
    "        integers = list(map(int, match[1][1:-1].split(',')))  # Extract integers within []\n",
    "        dd[doc_id] = integers\n",
    "        \n",
    "        \n",
    "    return dd '''\n",
    "\n",
    "def stringToDictionary(str):\n",
    "    # Regular expression pattern to match docId and integers\n",
    "    p = r'(\\d+):(\\[\\d+(?:,\\d+)*\\])'\n",
    "\n",
    "    # Find all matches in the input string\n",
    "    match = re.findall(p, str)\n",
    "\n",
    "    # Extract docId and integers from matches\n",
    "    extractedData = {doc_id: [int(n) for n in integers.split(',')] for doc_id, integers in match}\n",
    "    \n",
    "    return extractedData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8130685-7498-4cc6-94b7-891dae5e2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads string from inverted Index\n",
    "'''def extractDict(key, position, path):\n",
    "    start, length = position\n",
    "    \n",
    "    # read content from file on specific start, end index\n",
    "    with open(f'../Resources/{folder}/invertedIndex/{path}') as file:\n",
    "        file.seek(start)\n",
    "        line = file.read(length + 1)\n",
    "        \n",
    "    token = \"\"\n",
    "    i = 0\n",
    "\n",
    "    while i < len(line) and line[i] != ':':\n",
    "        token += line[i]\n",
    "        i += 1\n",
    "        \n",
    "    dd = stringToDictionary(line[len(token)+2:-1])\n",
    "    updatedDD = {}\n",
    "    \n",
    "    updatedDD[token] = dd\n",
    "    \n",
    "    return updatedDD '''\n",
    "\n",
    "def extractDict(key, position, path):\n",
    "    start, length = position\n",
    "    \n",
    "    # Read content from file with specific start and length\n",
    "    with open(f'../Resources/{folder}/invertedIndex/{path}') as file:\n",
    "        file.seek(start)\n",
    "        line = file.read(length + 1)\n",
    "        \n",
    "    token = line.split(\":\")[0]\n",
    "\n",
    "    # Extract the catalog string\n",
    "    catalog_string = line[len(token)+2:-1]\n",
    "\n",
    "    # Convert catalog string to dictionary\n",
    "    dd = stringToDictionary(catalog_string)\n",
    "    \n",
    "    # Create a dictionary with the token as key and the converted catalog as value\n",
    "    updatedDD = {token: dd}\n",
    "    \n",
    "    return updatedDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a8653-7bfb-4aa9-b90b-c44180b41e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns merged dictionaries based on catalogs, and index files.\n",
    "def mergeDict(c1, c2, i1, i2):\n",
    "    # Gets catalog data in string\n",
    "    catalog1 = openCatalog(c1)\n",
    "    catalog2 = openCatalog(c2)\n",
    "    \n",
    "    # Converts catalog string to keys and list of [start, length]\n",
    "    catalog1 = catalogToPList(catalog1)\n",
    "    catalog2 = catalogToPList(catalog2)\n",
    "    \n",
    "    finalDict = {}\n",
    "\n",
    "    # Loop over catalog1 keys, include which are part of both catalog1\n",
    "    # and catalog2\n",
    "    for k1 in catalog1.keys():\n",
    "        dict1 = extractDict(k1, catalog1[k1], i1)\n",
    "\n",
    "        if k1 in catalog2.keys():\n",
    "            dict2 = extractDict(k1, catalog2[k1], i2)\n",
    "            dict1[k1].update(dict2[k1])\n",
    "        \n",
    "        mergedDict[k1] = dict1[k1]\n",
    "\n",
    "    # Loop over catalog2 keys, include which are not part of catalog1\n",
    "    for k2 in catalog2.keys():\n",
    "        dict2 = extractDict(k2, catalog2[k2], i2)\n",
    "\n",
    "        if k2 not in catalog1.keys():\n",
    "            finalDict[k2] = dict2[k2]\n",
    "            \n",
    "    mergedDictStr = toString(mergedDict)\n",
    "    return mergedDict, getCatalogString(mergedDictStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fe06e-309d-484f-845c-a48b85ee7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumber(file):\n",
    "    # Extract the numeric part of the file name using regular expression\n",
    "    match = re.match(r'(\\d+)\\.txt', file)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "def performMerge(indexFile, catalogFile, mergeFile, file1, file2):\n",
    "    merged_dict, catalog = mergeDict(indexFile[file1], indexFile[file2], catalogFile[file1], catalogFile[file2])\n",
    "    merged_string = toString(merged_dict)\n",
    "    writeFile(merged_string, f'invertedIndex/{mergeFile}.txt')\n",
    "    writeFile(catalog, f'catalogs/{mergeFile}.txt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69968093-0111-4785-a4b7-023dff6aa37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def mergeFiles(indexPath, catalogPath):\n",
    "    indexFileList = sorted(os.listdir(indexPath), key=getNumber)\n",
    "    catalogFileList = sorted(os.listdir(catalogPath), key=getNumber)\n",
    "    \n",
    "    indexFileList = [name for name in indexFileList if name.endswith('.txt')]\n",
    "    catalogFileList = [name for name in catalogFileList if name.endswith('.txt')]\n",
    "    \n",
    "    n = len(indexFileList)\n",
    "    \n",
    "    while n > 1:\n",
    "        now = time.time()\n",
    "        print(\"\\n************************************\\n\")\n",
    "        print(f'n={n}')\n",
    "        \n",
    "        for i in range(0, n - 1, 2):\n",
    "            performMerge(indexFileList, catalogFileList, i//2, i, i + 1)\n",
    "\n",
    "        if n % 2:\n",
    "            performMerge(indexFileList, catalogFileList, (n//2)-1, (n//2)-1, n-1)\n",
    "\n",
    "        n //= 2 '''\n",
    "\n",
    "def merge(indexPath, catalogPath):\n",
    "    # Get sorted lists of index and catalog files\n",
    "    indexFileList = sorted(os.listdir(indexPath), key=getNumber)\n",
    "    catalogFileList = sorted(os.listdir(catalogPath), key=getNumber)\n",
    "    \n",
    "    # Filter out non-text files\n",
    "    indexFileList = [name for name in indexFileList if name.endswith('.txt')]\n",
    "    catalogFileList = [name for name in catalogFileList if name.endswith('.txt')]\n",
    "    \n",
    "    # Merge files in a pairwise manner until only one file is left\n",
    "    while len(indexFileList) > 1:\n",
    "        print(\"\\n************************************\\n\")\n",
    "        print(f'n={len(indexFileList)}')\n",
    "        \n",
    "        merged_indices = []\n",
    "        merged_catalogs = []\n",
    "        \n",
    "        for i in range(0, len(indexFileList) - 1, 2):\n",
    "            merged_index, merged_catalog = performMerge(indexFileList[i], indexFileList[i + 1], catalogFileList[i], catalogFileList[i + 1])\n",
    "            merged_indices.append(merged_index)\n",
    "            merged_catalogs.append(merged_catalog)\n",
    "        \n",
    "        if len(indexFileList) % 2:\n",
    "            merged_index, merged_catalog = performMerge(indexFileList[-1], \"\", catalogFileList[-1], \"\")\n",
    "            merged_indices.append(merged_index)\n",
    "            merged_catalogs.append(merged_catalog)\n",
    "        \n",
    "        indexFileList = merged_indices\n",
    "        catalogFileList = merged_catalogs\n",
    "\n",
    "        \n",
    "merge(f'../Resources/{folder}invertedIndex', f'../Resources/{folder}catalogs')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
